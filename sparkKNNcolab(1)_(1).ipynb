{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WSqCHZFrars"
   },
   "source": [
    "# Assignment - kNN implementation with Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx_y7uSHrart"
   },
   "source": [
    "In this assignment you will implement the kNN algorithm in Spark, and use your implementation to classify text documents. “Classification”\n",
    "is the task of labeling documents based upon their contents. You will be asked to perform 4 tasks, covering data preparation, feature extraction, and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDAu-mwXGQPC"
   },
   "source": [
    "## Collaboration\n",
    "\n",
    "If you worked with a partner on this assignment. Please indicate with whom you worked (names and netIds of all team members)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPeogFnvGfu8"
   },
   "outputs": [],
   "source": [
    "Michael Kelley, mpk7\n",
    "Jenny Bechtold, jlb14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnctulYAraru"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VWKbhtIraru"
   },
   "source": [
    "The dataset you will be using in this homework is the same as the dataset you used in the Spark introduction lab. That is the widely-used “20 newsgroups” dataset. A newsgroup post is like an old-school blog post, and this dataset has 19,997 such posts from 20 different categories, according to where the blog post was made. \n",
    "\n",
    "The 20 categories are listed in the file `news_categories.txt`. \n",
    "\n",
    "For each document, the category name can be extracted from the id of the document. For example, \n",
    "* the document with the id `20_newsgroups/comp.graphics/37261` is from the `comp.graphics` category, \n",
    "* the document with the id `20_newsgroups/sci.med/59082` is from the `sci.med` category. \n",
    "\n",
    "The data file has one line per document of text. It can be accessed at:\n",
    "\n",
    "`s3://s21risamyersbucket/lab/20_news_same_line.txt`\n",
    "\n",
    "We have also provided a small subset of the data in the file `20_news_same_line_random_sample.txt`, so that you can debug your Spark code on a small dataset, before running it on the entire dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozxmp_VctNim"
   },
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "This installs Apache Spark, Java, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark.\n",
    "[link text](https://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA-g1MgVtMOk",
    "outputId": "8cf5e682-07e4-46ad-e8ee-8b1697a013d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: apt-get: command not found\n",
      "/bin/bash: wget: command not found\n",
      "tar: Error opening archive: Failed to open 'spark-3.0.2-bin-hadoop2.7.tgz'\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
    "!tar -xvf spark-3.0.2-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFGhBMndtSxc"
   },
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Z3_GJ9otU5M"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTqd0pTxtXBv"
   },
   "source": [
    "## Start a pySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZqirhgT6tZ-i"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9edc9ba8077d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         raise Exception(\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l7XOcn1tdG7"
   },
   "source": [
    "## Load useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8ZoKCNDTraru"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GmGC0X_rarv"
   },
   "source": [
    "## (20 pts) Task 1 - compute \"bag of words\" for each document \n",
    "\n",
    "For task 1, we want to extract \"bag of words\" features for documents. \n",
    "\n",
    "The first part of this task is the same as what you've already implemented in lab. We need a dictionary, as an RDD, that includes the 20,000 most frequent words\n",
    "in the training corpus. The result of such an RDD must be in this format:\n",
    "`\n",
    "[('mostcommonword', 0),\n",
    " ('nextmostcommonword', 1),\n",
    " ...]\n",
    "`\n",
    "\n",
    "**NOTE**: There aren’t 20,000 unique words in the small dataset (`20_news_same_line_random_sample.txt`). Use only the top 50 words when working with this file.\n",
    "\n",
    "For this part, we provided our code, so that you only need to run it, to create this dictionary, named `refDict`, as an RDD. This `refDict` RDD will be our reference dictionary of words. The words in `refDict` will be our reference words for which we will compute \"bag of words\" and \"TF-IDF\" features for our training corpus and finally for the test documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKSS38Porarw"
   },
   "source": [
    "**Provided code to create the reference dictionary of words.**\n",
    "\n",
    "Run the code cells below to create the `refDict` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u5GiGF1zrarw"
   },
   "outputs": [],
   "source": [
    "# set the number of dictionary words \n",
    "# 50 for the small dataset\n",
    "# 20,000 for the large dataset\n",
    "numWords = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "awNpRREirarw"
   },
   "outputs": [],
   "source": [
    "# load up the dataset \n",
    "# \"20_news_same_line_random_sample.txt\" for small dataset \n",
    "# \"s3://s21risamyersbucket/lab/20_news_same_line.txt\" for entire large dataset\n",
    "corpus = sc.textFile (\"20_news_same_line_random_sample.txt\")\n",
    "\n",
    "# each entry in validLines will be a line from the text file\n",
    "validLines = corpus.filter(lambda x : 'id=' in x)\n",
    "\n",
    "# now we transform it into a bunch of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\"> ') + 3:x.index(' </doc>')]))\n",
    "\n",
    "# now we split the text in each (docID, text) pair into a list of words\n",
    "# after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# we have a bit of fancy regular expression stuff here to make sure that we do not\n",
    "# die on some of the documents                       \n",
    "regex = re.compile('[^a-zA-Z]')  \n",
    "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "# now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
    "\n",
    "# now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey (lambda a, b: a + b)\n",
    "\n",
    "# and get the top numWords (50 for small dataset, 20K for large dataset) frequent words in a local array\n",
    "topWords = allCounts.top (numWords, lambda x : x[1])\n",
    "\n",
    "# and we'll create an RDD that has a bunch of (word, rank) pairs\n",
    "# start by creating an RDD that has the number 0 up to numWords (50 for small dataset, 20K for large dataset) \n",
    "# numWords is the number of words that will be in our dictionary\n",
    "numWordsRDD = sc.parallelize(range(numWords))\n",
    "\n",
    "# now, we transform (0), (1), (2), ... to (\"mostcommonword\", 0) (\"nextmostcommon\", 1), ...\n",
    "# the number will be the spot in the dictionary used to tell us where the word is located\n",
    "refDict = numWordsRDD.map(lambda x:(topWords[x][0],x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AACNGsv1rarx",
    "outputId": "a8e4ab93-cb0c-42f8-938f-893f6aec8619",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('to', 1),\n",
       " ('of', 2),\n",
       " ('a', 3),\n",
       " ('and', 4),\n",
       " ('i', 5),\n",
       " ('in', 6),\n",
       " ('is', 7),\n",
       " ('that', 8),\n",
       " ('it', 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refDict.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsF6cLdYrary"
   },
   "source": [
    "Now, your task is to write Spark code, using RDDs, to create \"bag of words\" features based on the words in the reference dictionary, `refDict`.  \n",
    "\n",
    "You need to create a new RDD, named `bag_of_words`. Each element of this RDD corresponds to one document, and is a key-value pair. Specifically, the key is the document identifier `id` (e.g. `20_newsgroups/comp.graphics/37261`) and the value is a `numpy` array with `numWords` (50 for the small dataset, 20K for the large dataset) entries, where the *i*th entry in the array is the number of times that the *i*th word in the `refDict` (created in the first part) appears in the document. This array corresponds to the \"bag of words\" features for each document.  \n",
    "\n",
    "Once you created this `bag_of_words` RDD, print out the result arrays for these documents:\n",
    "* `20_newsgroups/soc.religion.christian/21626`\n",
    "* `20_newsgroups/talk.politics.misc/179019`\n",
    "* `20_newsgroups/rec.autos/103167`\n",
    "\n",
    "Since each array is going to be huge, with a lot of zeros, print out only the non-zero entries in the array (that is, for an array `a`, print out `a[a.nonzero()`]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L0xW0BTqrary"
   },
   "outputs": [],
   "source": [
    "# Makes RDD of word, docID for every word in every docID\n",
    "word_docID = keyAndListOfWords.flatMap(lambda line: [(j,line[0]) for j in line[1]])\n",
    "\n",
    "# Joins refDict with test2 to output RDD of (word's refDict index, docID))\n",
    "rDidx_docID = refDict.leftOuterJoin(word_docID).map(lambda line: line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ykbueWtUPgcE"
   },
   "outputs": [],
   "source": [
    "# Adds a 1 count after each (refDict_idx, docID)\n",
    "rDidx_docID_1count = rDidx_docID.map(lambda line: ((line),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "diaUraBGrsfp"
   },
   "outputs": [],
   "source": [
    "# Extracts docIDs and indexes from refDict to make sure every word is accounted for (even zero counts)\n",
    "docIDs = keyAndListOfWords.map(lambda x: x[0])\n",
    "index = refDict.map(lambda x: x[1])\n",
    "numberMapping = docIDs.cartesian(index)\n",
    "numberMapping1 = numberMapping.map(lambda x: ((x[1], x[0]), 0))\n",
    "\n",
    "# Unions the all numbers mapping with the docID words RDD to get all words in refDict with each docID accounted for\n",
    "new_initial_counts = numberMapping1.union(rDidx_docID_1count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pJ8aABuqsit0"
   },
   "outputs": [],
   "source": [
    "# Sums count of each (refDict_idx, docID)\n",
    "rDidx_docID_counts = new_initial_counts.reduceByKey(lambda a,b: a + b)\n",
    "\n",
    "# Switches order and unpacks columns to (docID, rDidx, counts)\n",
    "docID_rDidx_counts = rDidx_docID_counts.map(lambda x: (x[0][1], x[0][0], x[1])).sortBy(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gXowxcsfT91u"
   },
   "outputs": [],
   "source": [
    "# Groups everything by the docID to create docID: [array of word indexes and counts] \n",
    "bag_of_words = docID_rDidx_counts.groupBy(lambda line: line[0]).map(lambda x : (x[0], np.array([j[2] for j in x[1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bitax3nrarz"
   },
   "source": [
    "Print results by running the code cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVT8Pk3crarz",
    "outputId": "7cf21330-ab77-4e93-fbe7-bca7fef07d26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  2, 10,  4,  4,  5,  1,  6,  7,  8,  1,  1,  1,  3,  1,  2,  1,\n",
       "        1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1_1 = np.array(bag_of_words.lookup(\"20_newsgroups/soc.religion.christian/21626\"))\n",
    "arr1_1[arr1_1.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNqqSBV9rarz",
    "outputId": "54b942ff-325a-4c6f-85d1-6f954e5d5915"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 23,  5, 17,  6,  5, 14, 10,  3, 20, 15,  4, 11,  1,  1,  4,  4,\n",
       "        8,  4,  3,  2,  1,  2,  3, 10,  3,  1,  1,  1,  1,  1,  1,  2,  1,\n",
       "        1,  1,  2,  2,  2,  3])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1_2 = np.array(bag_of_words.lookup(\"20_newsgroups/talk.politics.misc/179019\"))\n",
    "arr1_2[arr1_2.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWb_fNMerar0",
    "outputId": "45e39cee-0926-47d6-f005-17fc644bb148"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 1, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1_3 = np.array(bag_of_words.lookup(\"20_newsgroups/rec.autos/103167\"))\n",
    "arr1_3[arr1_3.nonzero()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIfzl0YDrar0"
   },
   "source": [
    "## (25 pts) Task 2 - compute the TF-IDF value for each document \n",
    "\n",
    "It is often difficult to classify documents accurately using raw count vectors (bag of words). Thus, the next task is\n",
    "to write some more Spark RDD code that converts each of the count vectors to TF-IDF vectors. You need to create an RDD of key-value pairs, named `tfidf`, where the keys are document identifiers, and the values are the TF-IDF vector for that document. Again, we are only interested in the top `numWords` (50 for small dataset, 20K for large dataset) most common words.  \n",
    "\n",
    "The ith entry in a TF-IDF vector corresponds to the ith word in the top `numWords` most common words dictionary `refDict`. Then, the ith entry in a TF-IDF vector for document $d$ in a corpus with $D$ documents is computed as:\n",
    "\n",
    "$$ TF(i, d) \\times IDF(i, D) $$\n",
    "\n",
    "Where $TF(i, d)$ is: \n",
    "\n",
    "$$ \\frac {\\textrm{# of occurences of word $i$ in $d$}} {\\textrm{Total # of words in $d$}} $$\n",
    "\n",
    "Note that the “Total number of words” is not the number of distinct words. The “total number of words”\n",
    "in “Today is a great day today” is six. \n",
    "\n",
    "And the $IDF(i,D)$ is:\n",
    "\n",
    "$$ \\log \\frac {\\textrm{# of documents in corpus D}} {(\\textrm{# of documents having word $i$}) + 1} $$\n",
    "\n",
    "Once you created this `tfidf` RDD, print out the **non-zero** array entries (TF-IDF vector) that you have created for these documents:\n",
    "\n",
    "* `20_newsgroups/soc.religion.christian/21626`\n",
    "* `20_newsgroups/talk.politics.misc/179019`\n",
    "* `20_newsgroups/rec.autos/103167`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iV_yYgdtNKkR"
   },
   "outputs": [],
   "source": [
    "# TF\n",
    "tf = bag_of_words.map(lambda x: (x[0], [j / np.sum(x[1]) for j in x[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "HDKBaAvMrar0"
   },
   "outputs": [],
   "source": [
    "# IDF \n",
    "# Gets number of documents\n",
    "num_docs = bag_of_words.count()\n",
    "\n",
    "distinctWords = rDidx_docID_1count.map(lambda x: x[0]).distinct()\n",
    "groupedDistinct = distinctWords.groupByKey().map(lambda x : (x[0], np.array(list(x[1]))))\n",
    "countWord = groupedDistinct.map(lambda x: (x[0], np.count_nonzero(x[1]) + 1)).sortBy(lambda x: x[0])\n",
    "\n",
    "idfValue = countWord.map(lambda x: np.log(num_docs / x[1]))\n",
    "\n",
    "idf = np.array(idfValue.collect())\n",
    "tf_idf = tf.map(lambda x: (x[0], np.multiply(x[1], idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niEihIk8rar1"
   },
   "source": [
    "Once you created your `tfidf` RDD, print out the **non-zero** result arrays for these documents,\n",
    "* `20_newsgroups/soc.religion.christian/21626`\n",
    "* `20_newsgroups/talk.politics.misc/179019`\n",
    "* `20_newsgroups/rec.autos/103167`\n",
    "\n",
    "by running the code cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfR3439Urar1",
    "outputId": "1fee6980-eb1f-4dfb-f64d-77ed99c9aaa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.10988634e-03,  3.40904641e-03,  2.67266121e-02,  6.99609887e-03,\n",
       "        9.99334507e-03,  1.21776054e-02,  1.92857154e-03,  1.88627191e-02,\n",
       "        2.88594755e-02,  3.39860926e-02, -1.29805238e-05,  6.18732723e-03,\n",
       "        6.44074041e-03,  2.48828180e-02,  7.55332215e-03,  1.30097479e-02,\n",
       "        6.67746136e-03,  7.50693992e-03,  1.69851030e-02,  8.46760048e-03,\n",
       "       -1.29805238e-05, -1.29805238e-05,  5.20522259e-05,  9.45115098e-03,\n",
       "        1.08403993e-02,  2.03446507e-03,  1.21291645e-02,  1.40871349e-02,\n",
       "        1.29475147e-02])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2_1 = tf_idf.filter(lambda x: x[0]=='20_newsgroups/soc.religion.christian/21626').values()\n",
    "arr2_1 = arr2_1.collect()[0]\n",
    "arr2_1[arr2_1.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0KdKC83rar1",
    "outputId": "1fb334f3-66ef-47ee-f2a3-061deea9bdff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.28379247e-03,  1.46539349e-02,  4.99502216e-03,  1.11139483e-02,\n",
       "        5.60306483e-03,  3.60436913e-03,  1.64514653e-02,  1.54103996e-02,\n",
       "        4.76382731e-03,  5.23852517e-02,  2.31155993e-02,  1.01468132e-02,\n",
       "        3.04865293e-02, -4.85194336e-06,  2.31273882e-03,  9.62984488e-03,\n",
       "        1.24011455e-02,  2.14906889e-02,  1.12933166e-02,  7.29430038e-03,\n",
       "        4.99188859e-03,  3.74849703e-03,  5.61198421e-03,  6.96168642e-03,\n",
       "        3.16507397e-02,  1.29489621e-02, -4.85194336e-06, -4.85194336e-06,\n",
       "        1.94564146e-05,  2.90048565e-04,  1.62172384e-03,  4.57091524e-03,\n",
       "        8.10398781e-03,  7.60455389e-04,  4.53371683e-03,  4.15371898e-03,\n",
       "        8.79942137e-03,  1.05311591e-02,  5.78660650e-03,  1.31272153e-02])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2_2 = tf_idf.filter(lambda x: x[0]=='20_newsgroups/talk.politics.misc/179019').values()\n",
    "arr2_2 = arr2_2.collect()[0]\n",
    "arr2_2[arr2_2.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFd4w2Cqrar1",
    "outputId": "63fab9b6-bcb1-4914-ee38-92991b2760b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.18603676e-02,  2.57349582e-03,  7.54399579e-03,  1.10314779e-02,\n",
       "        5.82352974e-03,  4.74650120e-03,  6.22459276e-03,  6.22459276e-03,\n",
       "       -1.95980457e-05,  9.34165092e-03,  1.25227254e-02,  1.14040354e-02,\n",
       "        9.82108417e-03,  2.01633147e-02,  1.28220876e-02,  5.03333006e-02,\n",
       "       -1.95980457e-05, -1.95980457e-05,  7.85886549e-05,  1.17156871e-03,\n",
       "        6.55049239e-03,  3.07164333e-03,  1.73393664e-02,  1.89722358e-02,\n",
       "        5.10527487e-02,  3.06945299e-02,  2.74189597e-02,  1.05421473e-01,\n",
       "        3.08830683e-02,  5.90226626e-02,  3.17546715e-02,  3.79988623e-02,\n",
       "        3.53296040e-02,  2.94232060e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2_3 = tf_idf.filter(lambda x: x[0]=='20_newsgroups/rec.autos/103167').values()\n",
    "arr2_3 = arr2_3.collect()[0]\n",
    "arr2_3[arr2_3.nonzero()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwhVcWCrrar1"
   },
   "source": [
    "## (30 pts) Task 3 - build a kNN classifier \n",
    "\n",
    "Task 3 is to build a kNN classifier, as a Python function named `predictLabel` in the cell below. This function will take as input a text string (`test_doc`) and a number *k*, and then output the name of one of the 20 newsgroups. This name is the news group that the classifier thinks that the text string is “closest” to. It is computed using the classical kNN algorithm. \n",
    "\n",
    "Your function first needs to convert the input string into all **lower case** words, and then compute a TF-IDF vector corresponding to the words in `refDict` created in the first task. Recall that the words in `refDict` is our reference words to compute \"TF-IDF\" features. In task 2, we already computed TF-IDF values of these words for our training corpus. In this task, you need to compute the TF-IDF values of these words for the input text string `test_doc`. For that, you need to compute term frequency of these words in the `test_doc`.\n",
    "\n",
    "Since the IDF measure of a word only depends on the training corpus, and this measure is already calculated for `refDict` words in task 2, you don't need to re-calculate IDF for the `test_doc` and can re-use what you have.  \n",
    "\n",
    "Next, your function needs to find the *k* documents in the corpus that are “closest” to the `test_doc` (where distance is computed using the L2 norm between the TF-IDF feature vectors), and returns the newsgroup label that is most frequent in those top *k*. Ties go to the label with the closest corpus document. If there are multiple labels the same distance from the document, choose randomly. If you have the neighbors in a list, choosing the first of the tied values is fine, since there was some stochasticity in populating the list.\n",
    "\n",
    "Note that the numpy function `np.linalg.norm` computes the L2 norm of a vector. You can call it with the difference of the training document TF-IDF vectors and the test document TF-IDF vector. See the documentation here:[np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n",
    "\n",
    "Keep in mind that you should be leveraging Spark. Do NOT loop over the training documents. Instead, use your RDD with the TF-IDF vectors for each document.\n",
    "\n",
    "Once you have implemented your function, run it on the following 8 test cases. Each is an excerpt from a Wikipedia article, chosen to match one of the 20 newsgroups. By reading each test document, you can guess which of the 20 newsgroups is the most relevent topic, and you can compare that with what your prediction function returns. The result you get from the small dataset might not be so accurate, due to the small training corpus. But, once you run it on the entire dataset in S3, you should see reasonable results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "f9X76r42rar2"
   },
   "outputs": [],
   "source": [
    "# k is the number of neighbors to consider\n",
    "# test_doc is the text to compare \n",
    "def predictLabel (k, test_doc):\n",
    "    # your code here\n",
    "    regex = re.compile('[^a-zA-Z]')  \n",
    "    listOfDocWords = sc.parallelize(regex.sub(' ', test_doc).lower().split())\n",
    "    docWords1 = listOfDocWords.map(lambda x: (x, 1))\n",
    "\n",
    "    # Subsets only words in refDict\n",
    "    rD_docWords = refDict.join(docWords1)\n",
    "    rD_docWords2 = rD_docWords.map(lambda x: ((x[0], x[1][0]), x[1][1]))\n",
    "\n",
    "    # Counts of words in test_doc in form (wordIdx, counts)\n",
    "    docWords_counts = rD_docWords2.map(lambda x: (x[0][1], x[1]))\n",
    "\n",
    "    # RDD of all indexes in refDict to account for all words in refDict\n",
    "    index2 = refDict.map(lambda x: (x[1], 0))\n",
    "\n",
    "    rD_docWords_counts = docWords_counts.union(index2).reduceByKey(lambda a,b: a+b)\n",
    "    countsOnly = rD_docWords_counts.sortByKey().map(lambda x: x[1])\n",
    "    countList = countsOnly.collect()\n",
    "\n",
    "    # Compute TF for test doc\n",
    "    tfTest = countsOnly.map(lambda x: x / sum(countList))\n",
    "    tf_idfTest = np.array(tfTest.collect()) * idf\n",
    "\n",
    "    # k neighbors\n",
    "    tfidf_top_norms = sc.parallelize(tf_idf.map(lambda x: (x[0], np.linalg.norm(x[1] - tf_idfTest))).sortBy(lambda x: x[1]).take(k))\n",
    "    #tfidf_top_norms.collect()\n",
    "    # Gets the top categories from the docIDs, with the L2 norms\n",
    "    top_categories_norms = tfidf_top_norms.map(lambda x: (x[0].split('/')[1], x[1]))\n",
    "\n",
    "    # RDD of (category, [l2 norms]), sorted by most l2 norms\n",
    "    top_categories = top_categories_norms.groupBy(lambda x: x[0]).map(lambda x : (x[0], [j[1] for j in x[1]], len(x[1]))).sortBy(lambda x: x[2], ascending=False)\n",
    "\n",
    "    # Eliminating ties for top category\n",
    "    if top_categories.take(1)[0][2] == top_categories.take(2)[1][2]:\n",
    "      tied_len = top_categories.take(1)[0][2]\n",
    "      new_top_cat = top_categories.filter(lambda x: x[2] == tied_len).sortBy(lambda x: x[1][0])\n",
    "      top = new_top_cat.take(1)[0][0]\n",
    "    else:\n",
    "      top = top_categories.take(1)[0][0]\n",
    "\n",
    "\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Vdq8Swrar2"
   },
   "source": [
    "#### Test cases\n",
    "\n",
    "Run your predictLabel function on the 8 test cases below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gOtm4Sqrar3",
    "outputId": "19061da7-e026-4e79-b714-c2457864d29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'Graphics are pictures and movies created using computers – usually referring to image data created by a computer specifically with help from specialized graphical hardware and software. It is a vast and recent area in computer science. The phrase was coined by computer graphics researchers Verne Hudson and William Fetter of Boeing in 1960. It is often abbreviated as CG, though sometimes erroneously referred to as CGI. Important topics in computer graphics include user interface design, sprite graphics, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization with ray tracing, and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, and physics. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the user, and processing image data received from the physical world. The interaction and understanding of computers and interpretation of data has been made easier because of computer graphics. Computer graphic development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design generally.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWMZ7kbdrar3",
    "outputId": "fa8aa93f-d2ef-40e9-e376-3afc507e26f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'A deity is a concept conceived in diverse ways in various cultures, typically as a natural or supernatural being considered divine or sacred. Monotheistic religions accept only one Deity (predominantly referred to as God), polytheistic religions accept and worship multiple deities, henotheistic religions accept one supreme deity without denying other deities considering them as equivalent aspects of the same divine principle, while several non-theistic religions deny any supreme eternal creator deity but accept a pantheon of deities which live, die and are reborn just like any other being. A male deity is a god, while a female deity is a goddess. The Oxford reference defines deity as a god or goddess (in a polytheistic religion), or anything revered as divine. C. Scott Littleton defines a deity as a being with powers greater than those of ordinary humans, but who interacts with humans, positively or negatively, in ways that carry humans to new levels of consciousness beyond the grounded preoccupations of ordinary life.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lU7QbSLKrar3",
    "outputId": "252ede26-fa7e-454e-878d-025dde1a121b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'Egypt, officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia by a land bridge formed by the Sinai Peninsula. Egypt is a Mediterranean country bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south, and Libya to the west. Across the Gulf of Aqaba lies Jordan, and across from the Sinai Peninsula lies Saudi Arabia, although Jordan and Saudi Arabia do not share a land border with Egypt. It is the worlds only contiguous Eurafrasian nation. Egypt has among the longest histories of any modern country, emerging as one of the worlds first nation states in the tenth millennium BC. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypts rich cultural heritage is an integral part of its national identity, which has endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. One of the earliest centers of Christianity, Egypt was Islamised in the seventh century and remains a predominantly Muslim country, albeit with a significant Christian minority.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE9nun1Frar4",
    "outputId": "15968035-1d3c-4f38-d991-1da830c91f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'The term atheism originated from the Greek atheos, meaning without god(s), used as a pejorative term applied to those thought to reject the gods worshiped by the larger society. With the spread of freethought, skeptical inquiry, and subsequent increase in criticism of religion, application of the term narrowed in scope. The first individuals to identify themselves using the word atheist lived in the 18th century during the Age of Enlightenment. The French Revolution, noted for its unprecedented atheism, witnessed the first major political movement in history to advocate for the supremacy of human reason. Arguments for atheism range from the philosophical to social and historical approaches. Rationales for not believing in deities include arguments that there is a lack of empirical evidence; the problem of evil; the argument from inconsistent revelations; the rejection of concepts that cannot be falsified; and the argument from nonbelief. Although some atheists have adopted secular philosophies (eg. humanism and skepticism), there is no one ideology or set of behaviors to which all atheists adhere.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb6h4W8Crar4",
    "outputId": "3f64c7f1-a2d3-4417-d640-0f909b6b4816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'President Dwight D. Eisenhower established NASA in 1958 with a distinctly civilian (rather than military) orientation encouraging peaceful applications in space science. The National Aeronautics and Space Act was passed on July 29, 1958, disestablishing NASAs predecessor, the National Advisory Committee for Aeronautics (NACA). The new agency became operational on October 1, 1958. Since that time, most US space exploration efforts have been led by NASA, including the Apollo moon-landing missions, the Skylab space station, and later the Space Shuttle. Currently, NASA is supporting the International Space Station and is overseeing the development of the Orion Multi-Purpose Crew Vehicle, the Space Launch System and Commercial Crew vehicles. The agency is also responsible for the Launch Services Program (LSP) which provides oversight of launch operations and countdown management for unmanned NASA launches.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFLQ36A3rar5",
    "outputId": "fafccf1f-cbfb-45f4-f29d-4659e88dccbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'The transistor is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. First conceived by Julius Lilienfeld in 1926 and practically implemented in 1947 by American physicists John Bardeen, Walter Brattain, and William Shockley, the transistor revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The transistor is on the list of IEEE milestones in electronics, and Bardeen, Brattain, and Shockley shared the 1956 Nobel Prize in Physics for their achievement.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6CE_7Aurar5",
    "outputId": "d65d7ea7-3822-441a-8871-cdf6aa663289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'The Colt Single Action Army which is also known as the Single Action Army, SAA, Model P, Peacemaker, M1873, and Colt .45 is a single-action revolver with a revolving cylinder holding six metallic cartridges. It was designed for the U.S. government service revolver trials of 1872 by Colts Patent Firearms Manufacturing Company – todays Colts Manufacturing Company – and was adopted as the standard military service revolver until 1892. The Colt SAA has been offered in over 30 different calibers and various barrel lengths. Its overall appearance has remained consistent since 1873. Colt has discontinued its production twice, but brought it back due to popular demand. The revolver was popular with ranchers, lawmen, and outlaws alike, but as of the early 21st century, models are mostly bought by collectors and re-enactors. Its design has influenced the production of numerous other models from other companies.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mu-sueWcrar6",
    "outputId": "1efa965a-a8b7-4c74-a637-3db6a84bd9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(predictLabel (10, 'Howe was recruited by the Red Wings and made his NHL debut in 1946. He led the league in scoring each year from 1950 to 1954, then again in 1957 and 1963. He ranked among the top ten in league scoring for 21 consecutive years and set a league record for points in a season (95) in 1953. He won the Stanley Cup with the Red Wings four times, won six Hart Trophies as the leagues most valuable player, and won six Art Ross Trophies as the leading scorer. Howe retired in 1971 and was inducted into the Hockey Hall of Fame the next year. However, he came back two years later to join his sons Mark and Marty on the Houston Aeros of the WHA. Although in his mid-40s, he scored over 100 points twice in six years. He made a brief return to the NHL in 1979–80, playing one season with the Hartford Whalers, then retired at the age of 52. His involvement with the WHA was central to their brief pre-NHL merger success and forced the NHL to expand their recruitment to European talent and to expand to new markets.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n112P7jgrar6"
   },
   "source": [
    "## (15 pts) Task 4 - run on the entire dataset in EMR cluster \n",
    "\n",
    "For the last part of this homework, you need to run your Spark code for tasks 1 through 3, on the entire dataset stored in S3, using an AWS EMR cluster. \n",
    "\n",
    "Follow the instructions from `Lab - Spark Intro (AWS)` to create and connect to an EMR cluster in AWS and run Spark programs there. You can put your code for each task in a Python `.py` file and submit them as jobs in the batch mode and get the final result back. To troubleshoot, you can run your code line by line, in an interactive mode to debug your program.       \n",
    "\n",
    "The entire dataset exists in this S3 URI: `s3://s21risamyersbucket/lab/20_news_same_line.txt`\n",
    "\n",
    "Repeat tasks 1 through 3 on the entire dataset in your EMR cluster, and print your results in the markdown cells below (keep the results from the small subset above). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVCHohIwrar6"
   },
   "source": [
    "**Repeat task 1 on the entire dataset in your EMR cluster - print out the non-zero array entries (bag of words) that you have created for documents:**\n",
    "\n",
    "* `20_newsgroups/soc.religion.christian/21626`\n",
    "* `20_newsgroups/talk.politics.misc/179019`\n",
    "* `20_newsgroups/rec.autos/103167`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYHUwkp_rar6"
   },
   "source": [
    "array([ 2,  1,  2,  1,  2,  2,  1,  1,  3,  1,  1,  1,  5,  3,  1,  2,  1,\n",
    "        1,  1,  1,  1,  2,  3,  1,  1,  7,  2, 10,  4,  4,  5,  1,  6,  7,\n",
    "        8,  1,  1,  1,  1,  3,  2,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
    "        1,  3,  1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  2,  2,  1,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
    "        1,  2,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  3,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  2,  1,  2,  1,  1,  2,  1,  1,  1,  1,  2])\n",
    "\n",
    "array([ 1,  1,  1,  1,  1,  2,  2,  2,  1,  1,  1,  2,  2,  1,  5,  1,  2,\n",
    "        1,  4,  1,  2,  1,  1,  2,  1,  4,  3,  1,  1,  1,  7, 23,  5, 17,\n",
    "        6,  5, 14, 10,  3, 15, 20,  4,  1, 11,  1,  4,  8,  4,  4,  3,  2,\n",
    "        1,  2,  3, 10,  3,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  2,\n",
    "        2,  3,  5,  2,  2,  1,  1,  2,  1,  1,  8,  1,  2,  1,  3,  1,  1,\n",
    "        2,  1,  2,  1,  1,  2,  2,  3,  1,  1,  1,  1,  2,  1,  1, 11,  3,\n",
    "        1,  1,  1,  1,  1,  2,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "        1,  2,  2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  2,  1,  1,  6,  1,\n",
    "        3,  1,  3,  1,  1,  1,  3,  3,  2,  1,  1,  3, 11,  2,  1,  1,  1,\n",
    "        1, 11,  1,  1,  1,  1,  1,  1,  1,  2,  3,  1,  1,  1,  1,  1,  1,\n",
    "        1,  1,  1,  1,  1,  1,  1,  1])\n",
    "\n",
    "array([1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 9, 1, 2, 3, 2, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       2, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6nPnlrFrar6"
   },
   "source": [
    "**Repeat task 2 on the entire dataset in your EMR cluster - print out the non-zero array entries (TF-IDF) that you have created for documents:**\n",
    "\n",
    "* `20_newsgroups/soc.religion.christian/21626`\n",
    "* `20_newsgroups/talk.politics.misc/179019`\n",
    "* `20_newsgroups/rec.autos/103167`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1p1TIu8rar7"
   },
   "source": [
    "array([ 2.55502668e-03, 1.29010463e-03, 9.25293813e-03, 2.57906645e-03, 3.67581489e-03, 4.54955151e-03, 8.34284642e-04, 8.07815673e-03, 1.17952717e-02, 1.33848412e-02, -2.51287692e-07, 2.51413480e-03, 2.57081759e-03, 2.99842959e-03, 9.55455241e-03, 5.23511872e-03, 2.92728501e-03, 3.31244031e-03, 3.30807000e-03, 6.65602007e-03, -2.51287692e-07, 1.43442099e-05, -2.51287692e-07, 3.95742539e-03, 4.36932519e-03, 8.16219817e-04, 4.73739398e-03, 1.36238925e-02, 4.97312923e-03, 5.69998357e-03, 4.91532217e-03, 5.00500475e-03, 5.36527949e-03, 2.04797150e-02, 6.31505670e-03, 5.32306353e-03, 5.83217927e-03, 6.42034550e-03, 1.27544993e-02, 2.49103093e-02, 6.45745150e-03, 1.45971251e-02, 1.23867036e-02, 7.43064488e-03, 7.57378230e-03, 7.21696122e-03, 1.34638831e-02, 8.62610429e-03, 8.10575352e-03, 8.16792605e-03, 1.86790398e-02, 8.74068636e-03, 1.30526508e-02, 8.97350148e-03, 9.22392636e-03, 1.69735918e-02, 1.05831471e-02, 1.31790972e-02, 1.21190556e-02, 1.16027955e-02, 1.21443429e-02, 2.39507838e-02, 1.20494737e-02, 1.18383711e-02, 1.29856034e-02, 1.30594050e-02, 1.22669477e-02, 2.89226053e-02, 1.47084898e-02, 1.33659526e-02, 1.45468963e-02, 1.39884518e-02, 1.39965896e-02, 1.50031755e-02, 4.79141291e-02, 1.42900498e-02, 1.55882136e-02, 1.55659045e-02, 1.71335864e-02, 1.80300479e-02, 1.85162685e-02, 1.92825601e-02, 1.69105413e-02, 3.43128906e-02, 1.87519444e-02, 3.47818832e-02, 1.65527050e-02, 1.85868331e-02, 4.07728519e-02, 1.89444196e-02, 1.94005262e-02, 1.94364629e-02, 2.27077511e-02, 4.23075920e-02, 5.23818056e-02, 2.52670363e-02, 2.53443474e-02, 2.84885982e-02, 9.96105629e-02, 7.10565246e-02, 3.33931709e-02, 7.16658173e-02, 3.52410341e-02, 6.80185259e-02, 3.65039207e-02, 3.87241857e-02, 3.61572228e-02, 4.27992120e-02, 3.99870723e-02, 3.58329086e-02, 8.33557752e-02, 1.07498726e-01, 3.58329086e-02, 3.58329086e-02, 1.82519603e-01, 1.14584208e-01, 3.99870723e-02, 7.86321206e-02, 3.99870723e-02])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AnSPMIh98v"
   },
   "source": [
    "array([0.00881331, 0.01370039, 0.01044009, 0.02205 , 0.03316673, 0.01155318, 0.01113388, 0.01240744, 0.02263282, 0.08807844, 0.28802224, 0.06232167, 0.26024498, 0.06821326, 0.06058964, 0.16930716, 0.11775053, 0.03551871, 0.18176892, 0.3298152 , 0.04985734, 0.01246433, 0.1485993 , 0.01224492, 0.04847171, 0.11813273, 0.04710021, 0.05122377, 0.03585418, 0.02481488, 0.01235195, 0.02413815, 0.03987205, 0.11651323, 0.03765741, 0.01188363, 0.01186149, 0.01375089, 0.01167148, 0.01214276, 0.01402197, 0.02541502, 0.01177505, 0.01240744, 0.01177505, 0.01157247, 0.02701806, 0.02359271, 0.04140767, 0.07654264, 0.0234661 , 0.02418674, 0.01280594, 0.01171234, 0.02330265, 0.01216787, 0.01165132, 0.09948566, 0.01337549, 0.02385716, 0.01283974, 0.0368934 , 0.01204504, 0.01235195, 0.02504541, 0.01280594, 0.02522642, 0.01211793, 0.01273985, 0.02399546, 0.02683826, 0.0365036 , 0.01298023, 0.01186149, 0.01171234, 0.01227121, 0.02454241, 0.01301677, 0.01175395, 0.14199775, 0.04299086, 0.01224492, 0.01175395, 0.01270751, 0.01232471, 0.01287405, 0.02516527, 0.01261321, 0.02381199, 0.01204504, 0.01204504, 0.01360267, 0.01380256, 0.01252271, 0.01167148, 0.01280594, 0.01261321, 0.01195139, 0.01309169, 0.02516527, 0.0262602 , 0.01255247, 0.01277265, 0.01179635, 0.01232471, 0.01195139, 0.01232471, 0.02804394, 0.01186149, 0.01370039, 0.02385716, 0.01264421, 0.01181786, 0.07256021, 0.01216787, 0.0393903 , 0.01197444, 0.03538906, 0.01309169, 0.01211793, 0.01209337, 0.0415663 , 0.03592333, 0.02666537, 0.01232471, 0.01270751, 0.03802691, 0.13840898, 0.0262602 , 0.01216787, 0.01221894, 0.01309169, 0.01337549, 0.13943199, 0.01290889, 0.01301677, 0.01258263, 0.01267564, 0.01333269, 0.01301677, 0.06209083, 0.0393903 , 0.01329068, 0.01355536, 0.01468773, 0.02701806, 0.01632423, 0.01426514, 0.01822827, 0.01476659, 0.01520864, 0.01667063, 0.01588792, 0.01552271, 0.01530853, 0.01575947, 0.03233837, 0.03334126, 0.01863798, 0.01575947, 0.01602415, 0.03151895, 0.03827885, 0.01575947, 0.08012077, 0.01616919, 0.03373232, 0.01588792, 0.06409662, 0.01632423, 0.03177584, 0.01667063, 0.01588792, 0.01822827, 0.01602415, 0.01602415, 0.01649076, 0.01686616])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZijskhfNiBbW"
   },
   "source": [
    "array([0.07647283, 0.05482856, 0.05859618, 0.05922118, 0.18408538, 0.06262573, 0.06240402, 0.06116542, 0.06738187, 0.15295734, 0.15671655, 0.07351287, 0.07174122, 0.07488949, 0.07288788, 0.66760398, 0.07351287, 0.15295734, 0.21689585, 0.15888893, 0.07229862, 0.07288788, 0.07417822, 0.07944446, 0.07647867, 0.07565349, 0.07288788, 0.08065872, 0.15295734, 0.21689585, 0.07835827, 0.07565349, 0.07565349, 0.07647867, 0.07647867, 0.08065872, 0.07835827, 0.08362452, 0.07647867, 0.08203533, 0.07488949, 0.07737569, 0.07351287, 0.14835644, 0.07488949, 0.14977897, 0.07288788, 0.07351287, 0.08362452, 0.22696046, 0.07835827, 0.07835827, 0.07417822, 0.07565349, 0.07417822, 0.07565349, 0.09077036, 0.22253466, 0.07835827, 0.07737569, 0.07417822, 0.07351287, 0.07351287, 0.07417822, 0.07417822, 0.08362452, 0.07565349, 0.07944446, 0.08550412, 0.07417822, 0.07835827])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLCxDVnXrar7"
   },
   "source": [
    "**Repeat task 3 on the entire dataset in your EMR cluster - print out the predicted label for each of the below test document:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HYXkn4qrar7"
   },
   "outputs": [],
   "source": [
    "print(predictLabel (10, 'Graphics are pictures and movies created using computers – usually referring to image data created by a computer specifically with help from specialized graphical hardware and software. It is a vast and recent area in computer science. The phrase was coined by computer graphics researchers Verne Hudson and William Fetter of Boeing in 1960. It is often abbreviated as CG, though sometimes erroneously referred to as CGI. Important topics in computer graphics include user interface design, sprite graphics, vector graphics, 3D modeling, shaders, GPU design, implicit surface visualization with ray tracing, and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, and physics. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the user, and processing image data received from the physical world. The interaction and understanding of computers and interpretation of data has been made easier because of computer graphics. Computer graphic development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design generally.'))\n",
    "print(predictLabel (10, 'A deity is a concept conceived in diverse ways in various cultures, typically as a natural or supernatural being considered divine or sacred. Monotheistic religions accept only one Deity (predominantly referred to as God), polytheistic religions accept and worship multiple deities, henotheistic religions accept one supreme deity without denying other deities considering them as equivalent aspects of the same divine principle, while several non-theistic religions deny any supreme eternal creator deity but accept a pantheon of deities which live, die and are reborn just like any other being. A male deity is a god, while a female deity is a goddess. The Oxford reference defines deity as a god or goddess (in a polytheistic religion), or anything revered as divine. C. Scott Littleton defines a deity as a being with powers greater than those of ordinary humans, but who interacts with humans, positively or negatively, in ways that carry humans to new levels of consciousness beyond the grounded preoccupations of ordinary life.'))\n",
    "print(predictLabel (10, 'Egypt, officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia by a land bridge formed by the Sinai Peninsula. Egypt is a Mediterranean country bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south, and Libya to the west. Across the Gulf of Aqaba lies Jordan, and across from the Sinai Peninsula lies Saudi Arabia, although Jordan and Saudi Arabia do not share a land border with Egypt. It is the worlds only contiguous Eurafrasian nation. Egypt has among the longest histories of any modern country, emerging as one of the worlds first nation states in the tenth millennium BC. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypts rich cultural heritage is an integral part of its national identity, which has endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. One of the earliest centers of Christianity, Egypt was Islamised in the seventh century and remains a predominantly Muslim country, albeit with a significant Christian minority.'))\n",
    "print(predictLabel (10, 'The term atheism originated from the Greek atheos, meaning without god(s), used as a pejorative term applied to those thought to reject the gods worshiped by the larger society. With the spread of freethought, skeptical inquiry, and subsequent increase in criticism of religion, application of the term narrowed in scope. The first individuals to identify themselves using the word atheist lived in the 18th century during the Age of Enlightenment. The French Revolution, noted for its unprecedented atheism, witnessed the first major political movement in history to advocate for the supremacy of human reason. Arguments for atheism range from the philosophical to social and historical approaches. Rationales for not believing in deities include arguments that there is a lack of empirical evidence; the problem of evil; the argument from inconsistent revelations; the rejection of concepts that cannot be falsified; and the argument from nonbelief. Although some atheists have adopted secular philosophies (eg. humanism and skepticism), there is no one ideology or set of behaviors to which all atheists adhere.'))\n",
    "print(predictLabel (10, 'President Dwight D. Eisenhower established NASA in 1958 with a distinctly civilian (rather than military) orientation encouraging peaceful applications in space science. The National Aeronautics and Space Act was passed on July 29, 1958, disestablishing NASAs predecessor, the National Advisory Committee for Aeronautics (NACA). The new agency became operational on October 1, 1958. Since that time, most US space exploration efforts have been led by NASA, including the Apollo moon-landing missions, the Skylab space station, and later the Space Shuttle. Currently, NASA is supporting the International Space Station and is overseeing the development of the Orion Multi-Purpose Crew Vehicle, the Space Launch System and Commercial Crew vehicles. The agency is also responsible for the Launch Services Program (LSP) which provides oversight of launch operations and countdown management for unmanned NASA launches.'))\n",
    "print(predictLabel (10, 'The transistor is the fundamental building block of modern electronic devices, and is ubiquitous in modern electronic systems. First conceived by Julius Lilienfeld in 1926 and practically implemented in 1947 by American physicists John Bardeen, Walter Brattain, and William Shockley, the transistor revolutionized the field of electronics, and paved the way for smaller and cheaper radios, calculators, and computers, among other things. The transistor is on the list of IEEE milestones in electronics, and Bardeen, Brattain, and Shockley shared the 1956 Nobel Prize in Physics for their achievement.'))\n",
    "print(predictLabel (10, 'The Colt Single Action Army which is also known as the Single Action Army, SAA, Model P, Peacemaker, M1873, and Colt .45 is a single-action revolver with a revolving cylinder holding six metallic cartridges. It was designed for the U.S. government service revolver trials of 1872 by Colts Patent Firearms Manufacturing Company – todays Colts Manufacturing Company – and was adopted as the standard military service revolver until 1892. The Colt SAA has been offered in over 30 different calibers and various barrel lengths. Its overall appearance has remained consistent since 1873. Colt has discontinued its production twice, but brought it back due to popular demand. The revolver was popular with ranchers, lawmen, and outlaws alike, but as of the early 21st century, models are mostly bought by collectors and re-enactors. Its design has influenced the production of numerous other models from other companies.'))\n",
    "print(predictLabel (10, 'Howe was recruited by the Red Wings and made his NHL debut in 1946. He led the league in scoring each year from 1950 to 1954, then again in 1957 and 1963. He ranked among the top ten in league scoring for 21 consecutive years and set a league record for points in a season (95) in 1953. He won the Stanley Cup with the Red Wings four times, won six Hart Trophies as the leagues most valuable player, and won six Art Ross Trophies as the leading scorer. Howe retired in 1971 and was inducted into the Hockey Hall of Fame the next year. However, he came back two years later to join his sons Mark and Marty on the Houston Aeros of the WHA. Although in his mid-40s, he scored over 100 points twice in six years. He made a brief return to the NHL in 1979–80, playing one season with the Hartford Whalers, then retired at the age of 52. His involvement with the WHA was central to their brief pre-NHL merger success and forced the NHL to expand their recruitment to European talent and to expand to new markets.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUhd7QtqiFVs"
   },
   "source": [
    "comp.graphics \\\\\n",
    "soc.religion.christian \\\\\n",
    "talk.politics.mideast \\\\\n",
    "alt.atheism \\\\\n",
    "sci.space \\\\\n",
    "sci.electronics \\\\\n",
    "talk.politics.guns \\\\\n",
    "rec.sport.hockey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCyaCaxLd2xg"
   },
   "source": [
    "# Don't forget to stop your Spark context when you are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "A-WfygVld1Kr"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH2m1fkArar7"
   },
   "source": [
    "Copy all the predicted labels in this markdown cell ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDp5JrXLrar7"
   },
   "source": [
    "### Copyright ©2021 Christopher M Jermaine (cmj4@rice.edu), Risa B Myers  (rbm2@rice.edu), Marmar Orooji (marmar.orooji@rice.edu)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sparkKNNcolab(1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
